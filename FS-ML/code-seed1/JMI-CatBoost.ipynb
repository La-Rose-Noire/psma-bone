{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93edefdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features names: ['wavelet.HLL_glszm_ZonePercentage', 'original_glszm_LowGrayLevelZoneEmphasis', 'wavelet.LHH_firstorder_RootMeanSquared', 'wavelet.HHL_gldm_SmallDependenceHighGrayLevelEmphasis', 'wavelet.HHL_firstorder_Kurtosis', 'original_glcm_InverseVariance', 'wavelet.HHL_glszm_GrayLevelNonUniformityNormalized', 'wavelet.HLH_firstorder_Skewness', 'wavelet.LLH_glcm_MCC', 'wavelet.HLH_glszm_SmallAreaEmphasis']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mutual_info_score,  roc_curve, auc, confusion_matrix, classification_report, roc_auc_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "from mrmr import mrmr_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Read data\n",
    "#file path: CT --- RFs_CT_bin50\\6-4ADASYN-1; PET (original) --- RFs_PET1_bin0.25\\6-4ADASYN-1\\original; \n",
    "#           PET (pre-Combat or Limma) --- RFs_PET1_bin0.25\\6-4ADASYN-1\\pre-Combat or Limma; PET (standardized) --- RFs_PET2_bin0.05\\6-4ADASYN-1; \n",
    "train_data = pd.read_csv(r'C:\\Users\\37427\\Desktop\\github\\FS-ML\\RFs_PET1_bin0.25\\6-4ADASYN-1\\pre-Combat\\train.csv')\n",
    "test_data = pd.read_csv(r'C:\\Users\\37427\\Desktop\\github\\FS-ML\\RFs_PET1_bin0.25\\6-4ADASYN-1\\pre-Combat\\test.csv')\n",
    "\n",
    "X_train = train_data.iloc[:, 1:] \n",
    "y_train = train_data.iloc[:, 0]  \n",
    "X_test = test_data.iloc[:, 1:]   \n",
    "y_test = test_data.iloc[:, 0]\n",
    "\n",
    "# Feature Selector ---- JMI\n",
    "np.random.seed(0)\n",
    "def calculate_mutual_info(X, y):\n",
    "    return mutual_info_classif(X, y, discrete_features=False)\n",
    "\n",
    "def calculate_redundancy(X, selected_features, feature_idx, y):\n",
    "    redundancy = 0\n",
    "    for other_idx in selected_features:\n",
    "        mutual_info = mutual_info_classif(X.iloc[:, [feature_idx, other_idx]], y, discrete_features=False).sum()\n",
    "        redundancy += mutual_info\n",
    "    return redundancy\n",
    "\n",
    "def jmi_feature_selection(X, y, k):\n",
    "    if k <= 0 or k > X.shape[1]:\n",
    "        raise ValueError(\"k must be greater than 0 and less than or equal to the number of features in X\")\n",
    "    \n",
    "    selected_features = []\n",
    "    remaining_features = list(range(X.shape[1]))\n",
    "\n",
    "    while len(selected_features) < k:\n",
    "        best_feature = None\n",
    "        best_score = -np.inf\n",
    "\n",
    "        for feature_idx in remaining_features:\n",
    "            mutual_info = calculate_mutual_info(X.iloc[:, [feature_idx]], y)[0]\n",
    "            redundancy = calculate_redundancy(X, selected_features, feature_idx, y)\n",
    "            score = mutual_info - redundancy\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_feature = feature_idx\n",
    "\n",
    "        selected_features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "selected_feature_indices = jmi_feature_selection(X_train, y_train, 10)\n",
    "selected_features = [X_train.columns[i] for i in selected_feature_indices]\n",
    "print(\"Selected features names:\", selected_features)\n",
    "X_train_selected_top_ten = X_train[selected_features]\n",
    "X_test_selected_top_ten = X_test[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8ce64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Classifier --- categorical boosting\n",
    "np.random.seed(0)\n",
    "class WrappedCatBoost(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, iterations=100, learning_rate=0.1, depth=6, l2_leaf_reg=1, border_count=1, random_seed=0):\n",
    "        self.iterations = iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.depth = depth\n",
    "        self.l2_leaf_reg = l2_leaf_reg\n",
    "        self.border_count = border_count\n",
    "        self.random_seed = random_seed\n",
    "        self.model_ = None\n",
    "\n",
    "    def _prepare_params(self, params):\n",
    "        return {**self.get_params(), **params}\n",
    "\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        self.model_ = CatBoostClassifier(\n",
    "            iterations=self.iterations,\n",
    "            learning_rate=self.learning_rate,\n",
    "            depth=self.depth,\n",
    "            l2_leaf_reg=self.l2_leaf_reg,\n",
    "            border_count=self.border_count,\n",
    "            random_seed=self.random_seed,\n",
    "            verbose=False\n",
    "        )\n",
    "        self.model_.fit(X, y, **fit_params)\n",
    "        self.classes_ = np.unique(y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model_.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model_.predict_proba(X)\n",
    "    \n",
    "param_grid = {\n",
    "    'iterations': [100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'depth': [3, 6, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5],\n",
    "    'border_count': [16, 64, 128]\n",
    "}\n",
    "catboost = WrappedCatBoost()\n",
    "grid_search = GridSearchCV(estimator=catboost, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_selected_top_ten, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_train_pred_cv = cross_val_predict(best_model, X_train_selected_top_ten, y_train, cv=5)\n",
    "y_train_pred_proba_cv = cross_val_predict(best_model, X_train_selected_top_ten, y_train, cv=5, method='predict_proba')[:, 1]\n",
    "train_precision_cv, train_recall_cv, train_f1_cv, _ = precision_recall_fscore_support(y_train, y_train_pred_cv, average='binary')\n",
    "y_test_pred = best_model.predict(X_test_selected_top_ten)\n",
    "y_test_pred_proba = best_model.predict_proba(X_test_selected_top_ten)[:, 1]\n",
    "test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='binary')\n",
    "\n",
    "train_accuracy_cv = cross_val_score(best_model, X_train_selected_top_ten, y_train, cv=5, scoring='accuracy').mean()\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "conf_mat_train_cv = confusion_matrix(y_train, y_train_pred_cv)\n",
    "conf_mat_test = confusion_matrix(y_test, y_test_pred)\n",
    "sensitivity_train = conf_mat_train_cv[1, 1] / (conf_mat_train_cv[1, 1] + conf_mat_train_cv[1, 0])  # TP / (TP + FN)\n",
    "sensitivity_test = conf_mat_test[1, 1] / (conf_mat_test[1, 1] + conf_mat_test[1, 0])  # TP / (TP + FN)\n",
    "specificity_train = conf_mat_train_cv[0, 0] / (conf_mat_train_cv[0, 0] + conf_mat_train_cv[1, 0])\n",
    "specificity_test = conf_mat_test[0, 0] / (conf_mat_test[0, 0] + conf_mat_test[1, 0])\n",
    "ppv_train = conf_mat_train_cv[1, 1] / (conf_mat_train_cv[1, 1] + conf_mat_train_cv[0, 1])\n",
    "npv_train = conf_mat_train_cv[0, 0] / (conf_mat_train_cv[0, 0] + conf_mat_train_cv[1, 0])\n",
    "ppv_test = conf_mat_test[1, 1] / (conf_mat_test[1, 1] + conf_mat_test[0, 1])\n",
    "npv_test = conf_mat_test[0, 0] / (conf_mat_test[0, 0] + conf_mat_test[1, 0])\n",
    "train_precision = precision_score(y_train, y_train_pred_cv)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "train_recall = recall_score(y_train, y_train_pred_cv)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred_cv)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "print(f\"Sensitivity (Cross-Validation): {sensitivity_train:.4f}\")\n",
    "print(f\"Specificity (Cross-Validation): {specificity_train:.4f}\")\n",
    "print(f\"PPV (Cross-Validation): {ppv_train:.4f}\")\n",
    "print(f\"NPV (Cross-Validation): {npv_train:.4f}\")\n",
    "print(f\"Precision (Cross-Validation): {train_precision:.4f}\")\n",
    "print(f\"Recall (Cross-Validation): {train_recall:.4f}\")\n",
    "print(f\"F1 Score (Cross-Validation): {train_f1:.4f}\")\n",
    "print(f\"Train Accuracy (Cross-Validation): {train_accuracy_cv:.4f}\")\n",
    "print(f\"Sensitivity (Test): {sensitivity_test:.4f}\")\n",
    "print(f\"Specificity (Test): {specificity_test:.4f}\")\n",
    "print(f\"PPV (Test): {ppv_test:.4f}\")\n",
    "print(f\"NPV (Test): {npv_test:.4f}\")\n",
    "print(f\"Precision (Test): {test_precision:.4f}\")\n",
    "print(f\"Recall (Test): {test_recall:.4f}\")\n",
    "print(f\"F1 Score (Test): {test_f1:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba_cv)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_pred_proba)\n",
    "np.random.seed(0)\n",
    "def bootstrap_roc_auc(y_true, y_scores, n_bootstraps=1000):\n",
    "    auc_scores = np.zeros(n_bootstraps)\n",
    "    for i in range(n_bootstraps):\n",
    "        indices = np.random.choice(len(y_true), size=len(y_true))\n",
    "        auc_scores[i] = roc_auc_score(y_true[indices], y_scores[indices])\n",
    "    return auc_scores\n",
    "\n",
    "auc_scores_train = bootstrap_roc_auc(y_train, y_train_pred_proba_cv)\n",
    "auc_train_mean = np.mean(auc_scores_train)\n",
    "auc_train_std = np.std(auc_scores_train)\n",
    "auc_train_lower = norm.ppf(0.025, loc=auc_train_mean, scale=auc_train_std)\n",
    "auc_train_upper = norm.ppf(0.975, loc=auc_train_mean, scale=auc_train_std)\n",
    "print(f\"Cross-Validation AUC: {auc_train_mean:.4f} (95% CI: {auc_train_lower:.4f}, {auc_train_upper:.4f})\")\n",
    "\n",
    "auc_scores_test = bootstrap_roc_auc(y_test, y_test_pred_proba)\n",
    "auc_test_mean = np.mean(auc_scores_test)\n",
    "auc_test_std = np.std(auc_scores_test)\n",
    "auc_test_lower = norm.ppf(0.025, loc=auc_test_mean, scale=auc_test_std)\n",
    "auc_test_upper = norm.ppf(0.975, loc=auc_test_mean, scale=auc_test_std)\n",
    "print(f\"Test AUC: {auc_test_mean:.4f} (95% CI: {auc_test_lower:.4f}, {auc_test_upper:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc7ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (python3.7)",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
