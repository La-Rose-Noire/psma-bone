{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae5d10d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_CT_bin50\\CTbin50-norm.csv'\n",
      "Normalized dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_PET1_bin0.25\\PET1bin0.25-norm.csv'\n",
      "Normalized dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_PET2_bin0.05\\PET2bin0.05-norm.csv'\n",
      "Normalized dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\Baseline\\Baseline1-norm.csv'\n",
      "Normalized dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\Baseline\\Baseline2-norm.csv'\n"
     ]
    }
   ],
   "source": [
    "# The following operations are performed on the dataset with a random seed of 1.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Function to read and normalize data\n",
    "def read_and_normalize(file_path, save_path):\n",
    "    # Read data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Normalize features using StandardScaler, excluding the 'Label' column if it exists\n",
    "    if 'Label' in df.columns:\n",
    "        labels = df[['Label']]\n",
    "        df.iloc[:, 1:] = scaler.fit_transform(df.iloc[:, 1:])\n",
    "        df = pd.concat([labels, df.iloc[:, 1:]], axis=1)\n",
    "    else:\n",
    "        df.iloc[:, :] = scaler.fit_transform(df.iloc[:, :])\n",
    "    \n",
    "    # Save normalized dataset\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"Normalized dataset saved to '{save_path}'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Paths for reading and saving normalized datasets\n",
    "file_paths = {\n",
    "    'CT': r'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_CT_bin50\\HighICC-CTbin50.csv',\n",
    "    'PET1': r'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_PET1_bin0.25\\HighICC-PET1bin0.25.csv',\n",
    "    'PET2': r'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_PET2_bin0.05\\HighICC-PET2bin0.05.csv',\n",
    "    'Baseline1': r'C:\\Users\\37427\\Desktop\\github\\preprocessing\\Baseline\\HighICC-Baseline1.csv',\n",
    "    'Baseline2': r'C:\\Users\\37427\\Desktop\\github\\preprocessing\\Baseline\\HighICC-Baseline2.csv'\n",
    "}\n",
    "\n",
    "save_paths = {\n",
    "    'CT_norm': r'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_CT_bin50\\CTbin50-norm.csv',\n",
    "    'PET1_norm': r'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_PET1_bin0.25\\PET1bin0.25-norm.csv',\n",
    "    'PET2_norm': r'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_PET2_bin0.05\\PET2bin0.05-norm.csv',\n",
    "    'Baseline1_norm': r'C:\\Users\\37427\\Desktop\\github\\preprocessing\\Baseline\\Baseline1-norm.csv',\n",
    "    'Baseline2_norm': r'C:\\Users\\37427\\Desktop\\github\\preprocessing\\Baseline\\Baseline2-norm.csv'\n",
    "}\n",
    "\n",
    "# Process each dataset\n",
    "normalized_datasets = {}\n",
    "for key, file_path in file_paths.items():\n",
    "    normalized_datasets[key] = read_and_normalize(file_path, save_paths[f'{key}_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fe9a0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_CT_bin50\\6-4ADASYN-1\\train.csv'\n",
      "Testing dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_CT_bin50\\6-4ADASYN-1\\test.csv'\n",
      "Training dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_PET1_bin0.25\\6-4ADASYN-1\\original\\train.csv'\n",
      "Testing dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_PET1_bin0.25\\6-4ADASYN-1\\original\\test.csv'\n",
      "Training dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_PET2_bin0.05\\6-4ADASYN-1\\train.csv'\n",
      "Testing dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_PET2_bin0.05\\6-4ADASYN-1\\test.csv'\n",
      "Training dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\Baseline\\6-4ADASYN-1\\original\\train.csv'\n",
      "Testing dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\Baseline\\6-4ADASYN-1\\original\\test.csv'\n",
      "Training dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\Baseline\\6-4ADASYN-1\\standardized\\train.csv'\n",
      "Testing dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\Baseline\\6-4ADASYN-1\\standardized\\test.csv'\n"
     ]
    }
   ],
   "source": [
    "# The following operations are performed on the dataset with a random seed of 1.\n",
    "# Function to split dataset into training and testing sets, then save them in specified directories\n",
    "def split_and_save_dataset(df, base_dir, train_ratio=0.6, random_state=1):\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    # Split dataset into training and testing sets directly, assuming 'Label' is the first column\n",
    "    train_df, test_df = train_test_split(df, test_size=1-train_ratio, random_state=random_state)\n",
    "\n",
    "    # Define paths for saving the datasets within the new directory\n",
    "    train_path = os.path.join(base_dir, 'train.csv')\n",
    "    test_path = os.path.join(base_dir, 'test.csv')\n",
    "\n",
    "    # Save the training and testing datasets\n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    print(f\"Training dataset saved to '{train_path}'\")\n",
    "\n",
    "    test_df.to_csv(test_path, index=False)\n",
    "    print(f\"Testing dataset saved to '{test_path}'\")\n",
    "\n",
    "# Specify new directories for each dataset type\n",
    "new_dirs = {\n",
    "    'CT': r'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_CT_bin50\\6-4ADASYN-1',\n",
    "    'PET1': r'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_PET1_bin0.25\\6-4ADASYN-1\\original',\n",
    "    'PET2': r'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_PET2_bin0.05\\6-4ADASYN-1',\n",
    "    'Baseline1': r'C:\\Users\\37427\\Desktop\\github\\preprocessing\\Baseline\\6-4ADASYN-1\\original',\n",
    "    'Baseline2': r'C:\\Users\\37427\\Desktop\\github\\preprocessing\\Baseline\\6-4ADASYN-1\\standardized'\n",
    "}\n",
    "\n",
    "# Apply the function to each normalized dataset\n",
    "for key, df in normalized_datasets.items():\n",
    "    split_and_save_dataset(df, new_dirs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a812fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced train dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_CT_bin50\\6-4ADASYN-1\\train_ADASYN.csv'\n",
      "0    110\n",
      "1     54\n",
      "Name: Label, dtype: int64\n",
      "Original minority count: 54\n",
      "Balanced minority count: 97\n",
      "Synthetic samples count: 43\n",
      "Balanced train dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_PET1_bin0.25\\6-4ADASYN-1\\original\\train_ADASYN.csv'\n",
      "0    110\n",
      "1     54\n",
      "Name: Label, dtype: int64\n",
      "Original minority count: 54\n",
      "Balanced minority count: 102\n",
      "Synthetic samples count: 48\n",
      "Balanced train dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_PET2_bin0.05\\6-4ADASYN-1\\train_ADASYN.csv'\n",
      "0    110\n",
      "1     54\n",
      "Name: Label, dtype: int64\n",
      "Original minority count: 54\n",
      "Balanced minority count: 104\n",
      "Synthetic samples count: 50\n",
      "Balanced train dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\Baseline\\6-4ADASYN-1\\original\\train_ADASYN.csv'\n",
      "0    110\n",
      "1     54\n",
      "Name: Label, dtype: int64\n",
      "Original minority count: 54\n",
      "Balanced minority count: 108\n",
      "Synthetic samples count: 54\n",
      "Balanced train dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\Baseline\\6-4ADASYN-1\\standardized\\train_ADASYN.csv'\n",
      "0    110\n",
      "1     54\n",
      "Name: Label, dtype: int64\n",
      "Original minority count: 54\n",
      "Balanced minority count: 101\n",
      "Synthetic samples count: 47\n",
      "Balanced train dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_CT_bin50\\6-4ADASYN-1\\train_ADASYN.csv'\n",
      "0    110\n",
      "1     54\n",
      "Name: Label, dtype: int64\n",
      "Original minority count: 54\n",
      "Balanced minority count: 97\n",
      "Synthetic samples count: 43\n",
      "Balanced train dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_PET1_bin0.25\\6-4ADASYN-1\\original\\train_ADASYN.csv'\n",
      "0    110\n",
      "1     54\n",
      "Name: Label, dtype: int64\n",
      "Original minority count: 54\n",
      "Balanced minority count: 102\n",
      "Synthetic samples count: 48\n",
      "Balanced train dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_PET2_bin0.05\\6-4ADASYN-1\\train_ADASYN.csv'\n",
      "0    110\n",
      "1     54\n",
      "Name: Label, dtype: int64\n",
      "Original minority count: 54\n",
      "Balanced minority count: 104\n",
      "Synthetic samples count: 50\n",
      "Balanced train dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\Baseline\\6-4ADASYN-1\\original\\train_ADASYN.csv'\n",
      "0    110\n",
      "1     54\n",
      "Name: Label, dtype: int64\n",
      "Original minority count: 54\n",
      "Balanced minority count: 108\n",
      "Synthetic samples count: 54\n",
      "Balanced train dataset saved to 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\Baseline\\6-4ADASYN-1\\standardized\\train_ADASYN.csv'\n",
      "0    110\n",
      "1     54\n",
      "Name: Label, dtype: int64\n",
      "Original minority count: 54\n",
      "Balanced minority count: 101\n",
      "Synthetic samples count: 47\n",
      "Trimmed 5 synthetic samples from 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_PET1_bin0.25\\6-4ADASYN-1\\original\\train_ADASYN.csv'\n",
      "Trimmed 7 synthetic samples from 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\RFs_PET2_bin0.05\\6-4ADASYN-1\\train_ADASYN.csv'\n",
      "Trimmed 11 synthetic samples from 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\Baseline\\6-4ADASYN-1\\original\\train_ADASYN.csv'\n",
      "Trimmed 4 synthetic samples from 'C:\\Users\\37427\\Desktop\\github\\preprocessing\\Baseline\\6-4ADASYN-1\\standardized\\train_ADASYN.csv'\n"
     ]
    }
   ],
   "source": [
    "# The following operations are performed on the dataset with a random seed of 1.\n",
    "# Function to apply ADASYN oversampling and save the balanced dataset\n",
    "def apply_adasyn_and_save(train_file_path, save_balanced_train_path):\n",
    "    # Load the training data\n",
    "    train_data = pd.read_csv(train_file_path)\n",
    "    \n",
    "    # Separate features and labels\n",
    "    X_train = train_data.drop('Label', axis=1)\n",
    "    y_train = train_data['Label']\n",
    "\n",
    "    # Initialize ADASYN without setting sampling_strategy; it will automatically generate samples based on classification difficulty\n",
    "    adasyn = ADASYN(random_state=1)\n",
    "\n",
    "    # Apply ADASYN oversampling\n",
    "    X_resampled, y_resampled = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Create a DataFrame for the balanced training set\n",
    "    train_df_balanced = pd.concat([\n",
    "        pd.DataFrame(y_resampled, columns=['Label']),\n",
    "        pd.DataFrame(X_resampled, columns=X_train.columns)\n",
    "    ], axis=1)\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(save_balanced_train_path), exist_ok=True)\n",
    "\n",
    "    # Save the balanced training set\n",
    "    train_df_balanced.to_csv(save_balanced_train_path, index=False)\n",
    "    print(f\"Balanced train dataset saved to '{save_balanced_train_path}'\")\n",
    "\n",
    "    # Calculate the number of synthetic samples generated\n",
    "    original_minority_count = y_train.value_counts().get(1, 0)  # Count only the minority class (label 1) samples\n",
    "    balanced_minority_count = y_resampled.value_counts().get(1, 0)  # Count only the minority class after balancing\n",
    "    synthetic_count = balanced_minority_count - original_minority_count\n",
    "\n",
    "    label_counts = y_train.value_counts()\n",
    "    print(label_counts)\n",
    "    print(f\"Original minority count: {original_minority_count}\")\n",
    "    print(f\"Balanced minority count: {balanced_minority_count}\")\n",
    "    print(f\"Synthetic samples count: {synthetic_count}\")\n",
    "\n",
    "# Apply ADASYN and save to new directories\n",
    "for key in file_paths:\n",
    "    base_dir = new_dirs[key]  # Use the new directories defined earlier\n",
    "    train_file_path = os.path.join(base_dir, 'train.csv')\n",
    "    balanced_train_path = os.path.join(base_dir, 'train_ADASYN.csv')\n",
    "\n",
    "    apply_adasyn_and_save(train_file_path, balanced_train_path)\n",
    "\n",
    "# Function to count the number of synthetic samples in a balanced dataset\n",
    "def count_synthetic_samples(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df['Label'].value_counts().get(1, 0)  # Assuming '1' is the minority class label\n",
    "\n",
    "# Function to trim excess synthetic samples from a dataset\n",
    "def trim_excess_synthetic_samples(file_path, target_synthetic_count):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Identify the indices of the synthetic samples (assuming they are at the end)\n",
    "    synthetic_indices = df[df['Label'] == 1].index\n",
    "    \n",
    "    # Calculate how many samples to remove\n",
    "    excess_count = len(synthetic_indices) - target_synthetic_count\n",
    "    \n",
    "    if excess_count > 0:\n",
    "        # Remove the last `excess_count` synthetic samples\n",
    "        df.drop(synthetic_indices[-excess_count:], inplace=True)\n",
    "        \n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        \n",
    "        # Save the trimmed DataFrame back to CSV\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Trimmed {excess_count} synthetic samples from '{file_path}'\")\n",
    "\n",
    "# Apply ADASYN and save, then find minimum synthetic sample count\n",
    "synthetic_sample_counts = {}\n",
    "for key in file_paths:\n",
    "    base_dir = new_dirs[key]\n",
    "    train_file_path = os.path.join(base_dir, 'train.csv')\n",
    "    balanced_train_path = os.path.join(base_dir, 'train_ADASYN.csv')\n",
    "\n",
    "    apply_adasyn_and_save(train_file_path, balanced_train_path)\n",
    "    \n",
    "    # Count synthetic samples for each file\n",
    "    synthetic_sample_counts[balanced_train_path] = count_synthetic_samples(balanced_train_path)\n",
    "\n",
    "# Find the minimum number of synthetic samples among all files\n",
    "min_synthetic_count = min(synthetic_sample_counts.values())\n",
    "\n",
    "# Trim excess synthetic samples for all files to match the minimum count\n",
    "for file_path, synthetic_count in synthetic_sample_counts.items():\n",
    "    if synthetic_count > min_synthetic_count:\n",
    "        trim_excess_synthetic_samples(file_path, min_synthetic_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae55f851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: 908 features remaining\n",
      "Iteration 2: 110 features remaining\n",
      "Process completed.\n",
      "Iteration 1: 1006 features remaining\n",
      "Iteration 2: 77 features remaining\n",
      "Process completed.\n",
      "Iteration 1: 967 features remaining\n",
      "Iteration 2: 102 features remaining\n",
      "Process completed.\n",
      "Iteration 1: 6 features remaining\n",
      "Iteration 2: 4 features remaining\n",
      "Process completed.\n",
      "Iteration 1: 6 features remaining\n",
      "Iteration 2: 4 features remaining\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "# The following operations are performed on the dataset with a random seed of 1.\n",
    "# Function to reduce features based on Spearman correlation\n",
    "def reduce_features_by_spearman(train_file_path, test_file_path, output_train_path, output_test_path):\n",
    "    # Read CSV files\n",
    "    train = pd.read_csv(train_file_path)\n",
    "    test = pd.read_csv(test_file_path)\n",
    "\n",
    "    # Assume the first column is the label column, and the rest are feature columns\n",
    "    label_column = 'Label'  # Explicitly specify the label column name\n",
    "    features = [col for col in train.columns if col != label_column]\n",
    "\n",
    "    # Initialize a set to store columns that need to be dropped\n",
    "    columns_to_drop = set()\n",
    "\n",
    "    # Set maximum iterations\n",
    "    max_iterations = 100\n",
    "\n",
    "    # Iterate until no pairs of features have a Spearman correlation greater than 0.7 or reach max iterations\n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"Iteration {iteration + 1}: {len(features)} features remaining\")\n",
    "\n",
    "        if len(features) <= 1:\n",
    "            break\n",
    "\n",
    "        # Step 2: Calculate Spearman correlation matrix among features\n",
    "        corr_matrix = train[features].corr(method='spearman')\n",
    "\n",
    "        # Step 2.1: Find pairs of features with correlation greater than 0.7\n",
    "        upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        high_corr_pairs = [(col1, col2) for col1, col2 in zip(*np.where(upper_triangle > 0.7))]\n",
    "\n",
    "        # If no highly correlated pairs found, exit loop\n",
    "        if not high_corr_pairs:\n",
    "            break\n",
    "\n",
    "        # Step 3: For each pair of highly correlated features, retain the one more correlated with the label\n",
    "        new_columns_to_drop = set()\n",
    "        for idx1, idx2 in high_corr_pairs:\n",
    "            col1, col2 = features[idx1], features[idx2]\n",
    "\n",
    "            # Calculate correlation with label\n",
    "            corr_with_label_col1 = abs(train[col1].corr(train[label_column], method='spearman'))\n",
    "            corr_with_label_col2 = abs(train[col2].corr(train[label_column], method='spearman'))\n",
    "\n",
    "            # Retain the feature more correlated with the label\n",
    "            if corr_with_label_col1 > corr_with_label_col2:\n",
    "                new_columns_to_drop.add(col2)\n",
    "            else:\n",
    "                new_columns_to_drop.add(col1)\n",
    "\n",
    "        # Update the set of columns to drop\n",
    "        columns_to_drop.update(new_columns_to_drop)\n",
    "\n",
    "        # Update the list of features, removing columns to drop\n",
    "        features = [col for col in features if col not in columns_to_drop]\n",
    "\n",
    "        # Drop these columns from training and testing datasets\n",
    "        train = train.drop(columns=new_columns_to_drop)\n",
    "        test = test.drop(columns=new_columns_to_drop)\n",
    "\n",
    "    print(\"Process completed.\")\n",
    "\n",
    "    # Ensure directories exist before saving\n",
    "    os.makedirs(os.path.dirname(output_train_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(output_test_path), exist_ok=True)\n",
    "\n",
    "    # Save modified training and testing datasets to new CSV files\n",
    "    train.to_csv(output_train_path, index=False)\n",
    "    test.to_csv(output_test_path, index=False)\n",
    "\n",
    "# Process all specified datasets\n",
    "for key in file_paths:\n",
    "    base_dir = new_dirs[key]  # Use the new directories defined earlier\n",
    "    \n",
    "    # Define paths for ADASYN balanced training and testing datasets\n",
    "    train_file_path = os.path.join(base_dir, 'train_ADASYN.csv')\n",
    "    test_file_path = os.path.join(base_dir, 'test.csv')\n",
    "    \n",
    "    # Define output paths for Spearman-reduced datasets within the same directory\n",
    "    output_train_path = os.path.join(base_dir, 'train_Spearman.csv')\n",
    "    output_test_path = os.path.join(base_dir, 'test_Spearman.csv')\n",
    "\n",
    "    # Apply feature reduction by Spearman correlation\n",
    "    reduce_features_by_spearman(train_file_path, test_file_path, output_train_path, output_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e11190b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (python3.7)",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
